{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook_python.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtWixxS9quDz"
      },
      "outputs": [],
      "source": [
        "##### pacotes necessários\n",
        "!pip install PyMuPDF\n",
        "!pip install PyPDF2\n",
        "!pip install spacy --upgrade\n",
        "!pip install xgboost\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "\n",
        "## download pipeline em português\n",
        "!python -m spacy download pt_core_news_lg\n",
        "\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import fitz\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import spacy\n",
        "import string\n",
        "import torch\n",
        "import seaborn as sns\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy import displacy\n",
        "from spacy.lang.pt.examples import sentences\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from transformers import AutoTokenizer  \n",
        "from transformers import AutoModelForPreTraining  \n",
        "from transformers import AutoModel\n",
        "from transformers import BertTokenizer\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBClassifier\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cdist as scipy_cdist\n",
        "from pathlib import Path \n",
        "%matplotlib inline\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from nltk.corpus import stopwords\n",
        "from pandas.core.frame import DataFrame\n",
        "from wordcloud import WordCloud\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy import displacy\n",
        "from spacy.lang.pt.examples import sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##criação tag - pipeline Spacy\n",
        "tag = spacy.load(\"pt_core_news_lg\")"
      ],
      "metadata": {
        "id": "1QM71zjAtSq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pasta com pdfs - montar drive antes\n",
        "# para montar o drive, basta clicar em arquivos e depois no link do google drive\n",
        "# as instruções são claras a partir daí \n",
        "path = \"/content/drive/MyDrive/...\"\n",
        "%cd \"/content/drive/MyDrive/...\""
      ],
      "metadata": {
        "id": "oFwEWL2ltY-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - pdfs contidos na pasta\n",
        "list_files = listdir(path)\n",
        "list_files"
      ],
      "metadata": {
        "id": "41GzZPAztZA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## nome dos pdfs\n",
        "files = [join(\"/content/drive/MyDrive/...\", f) for f in list_files]\n",
        "files"
      ],
      "metadata": {
        "id": "A_XY9bv6tZC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##loop para leitura dos textos\n",
        "textos = []\n",
        "for f in files:\n",
        "  with fitz.open(f) as doc:\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "  textos.append(text)"
      ],
      "metadata": {
        "id": "-EI0vFVutZE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - tamanho do objeto textos\n",
        "len(textos)"
      ],
      "metadata": {
        "id": "gqK-Xnq0tZHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto textos\n",
        "textos"
      ],
      "metadata": {
        "id": "2Ubv_MaOtZJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### limpeza básica\n",
        "## removendo espaços em branco\n",
        "textos = [f.strip() for f in textos]\n",
        "textos = [\" \".join(f.split()) for f in textos]"
      ],
      "metadata": {
        "id": "WkJJLtubtaGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## lowercase, remoção caracteres especiais, palavras pequenas e grandes demais, \n",
        "## números desnecessários\n",
        "textos = [f.lower() for f in textos]\n",
        "textos = [re.sub(':', ' ', f) for f in textos]\n",
        "textos = [f.strip('.,') for f in textos]\n",
        "textos = [f.replace('.',' ') for f in textos]\n",
        "textos = [f.replace('-',' ') for f in textos]\n",
        "textos = [f.replace('/',' ') for f in textos]\n",
        "textos = [re.sub(r'[0-9]+', '', f) for f in textos]\n",
        "textos = [f.replace('º',' ') for f in textos]\n",
        "textos = [f.replace('+',' ') for f in textos]\n",
        "textos = [f.replace('*',' ') for f in textos]\n",
        "textos = [f.replace('#',' ') for f in textos]\n",
        "textos = [f.replace('@',' ') for f in textos]\n",
        "textos = [f.replace('&',' ') for f in textos]\n",
        "textos = [f.replace('–',' ') for f in textos]\n",
        "textos = [f.replace(',',' ') for f in textos]\n",
        "textos = [f.replace(';',' ') for f in textos]\n",
        "textos = [f.replace(')',' ') for f in textos]\n",
        "textos = [f.replace('(',' ') for f in textos]\n",
        "textos = [f.replace('_',' ') for f in textos]\n",
        "textos = [f.replace(']',' ') for f in textos]\n",
        "textos = [f.replace('{',' ') for f in textos]\n",
        "textos = [f.replace('}',' ') for f in textos]\n",
        "textos = [f.replace('[',' ') for f in textos]\n",
        "textos = [f.replace('%',' ') for f in textos]\n",
        "textos = [f.replace('§',' ') for f in textos]\n",
        "textos = [f.replace('“',' ') for f in textos]\n",
        "textos = [f.replace('”',' ') for f in textos]\n",
        "textos = [f.replace('ª',' ') for f in textos]\n",
        "textos = [f.replace('°',' ') for f in textos]\n",
        "textos = [re.sub(r'\\b\\w{1,3}\\b', ' ', f) for f in textos]\n",
        "textos = [re.sub(r'\\b\\w{15,}\\b', ' ', f) for f in textos]\n",
        "textos = [f.strip() for f in textos]\n",
        "textos = [\" \".join(f.split()) for f in textos]"
      ],
      "metadata": {
        "id": "8L1vZEgWtaI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto textos\n",
        "textos"
      ],
      "metadata": {
        "id": "cqa9i-s-taLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## combinação - stopwords NLTK com outras palavras de parada\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "palavras_retirar = [\"página\",\"autenticacao\",\"documento\",\"e\",\"podem\",\"que\",\"de\",\n",
        "                    \"abr\",\"jan\",\"mar\",\"mai\",\"jun\",\"ago\",\"set\",\"out\",\"nov\",\"dez\",\n",
        "                    \"fev\",\"jul\",\"a\",\"o\",\"dele\",\"ingresso\",\"geral\",\"dela\",\n",
        "                    \"ele\",\"ela\",\"as\", \"os\", \"à\", \"mais\", \"nos\",\"por\",\"senhora\",\n",
        "                    \"às\",\"na\",\"no\",\"porque\",\"porquê\",\"por quê\",\"por que\", \n",
        "                    \"qual\",\"quais\",\"isso\",\"se\",\"por\",\"ainda\",\"ou\",\"das\", \n",
        "                    \"dos\",\"só\",\"em\",\"com\",\"esta\",\"está\",\"da\",\"do\",\"dar\",\n",
        "                    \"dá\",\"para\",\"fins\",\"sim\",\"não\",\"seu\",\"sua\",\"meu\", \n",
        "                    \"nosso\",\"nossa\",\"esse\",\"este\",\"aquele\",\"aquela\",\"aquilo\",\n",
        "                    \"temos\",\"base\",\"como\",\"quando\",\"aqueles\",\"aquelas\",\"nossos\",\n",
        "                    \"nossas\",\"meus\",\"qual\",\"firme\",\"pode\",\"causar\",\"caput\",\n",
        "                    \"casos\",\"caso\",\"inteiro\",\"teor\", \"portal\",\"eletrônico\",\n",
        "                    \"institui\",\"agravo\",\"regimental\",\"assinado\",\"chaves\",\n",
        "                    \"ementa\",\"acessado\",\"portal\",\"número\",\"direta\",\"para\",\n",
        "                    \"como\",\"endereço\",\"http\",\"digitalmente\",\"sido\",\"senhor\"]\n",
        "palavras_retirar = set(palavras_retirar)\n",
        "stop_words.update(palavras_retirar)"
      ],
      "metadata": {
        "id": "TiFgyv_WtaRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## remoção de stopwords\n",
        "textos = [word_tokenize(f) for f in textos]\n",
        "textos_limpos = []\n",
        "for f in textos:\n",
        "  text = [word for word in f if word not in stop_words]\n",
        "  text = \" \".join(text)\n",
        "  textos_limpos.append(text)"
      ],
      "metadata": {
        "id": "t6zXLxfPtaU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verifição - objeto textos_limpos\n",
        "textos_limpos"
      ],
      "metadata": {
        "id": "HfQlu06ZtaYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## opcional - truncar textos_limpos (10000 palavras, por exemplo)\n",
        "#se for uma opção, usar o objeto textos_trunc a partir daqui\n",
        "textos_trunc = [\" \".join(f.split()[:10000]) for f in textos_limpos]"
      ],
      "metadata": {
        "id": "RVZ1mD-Mtaac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## dataframe inicial - usar textos_limpos ou textos_trunc\n",
        "list_files_limpo = [re.sub('.pdf', '', f) for f in list_files]\n",
        "list_files_limpo = [re.sub('_', '', f) for f in list_files_limpo]\n",
        "data = pd.DataFrame({'ID':list_files_limpo, 'textos':textos_trunc})"
      ],
      "metadata": {
        "id": "um2tHyZMtac-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto data\n",
        "data"
      ],
      "metadata": {
        "id": "II8mFMYvtafS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - texto 1\n",
        "data['textos'][0]"
      ],
      "metadata": {
        "id": "QGl6pq42taho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - qtd caracteres texto 1\n",
        "len(data['textos'][0])"
      ],
      "metadata": {
        "id": "SsQXxpIhtaj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## opcional - pos tag para manter verbos e nomes\n",
        "## ou outras classes de interesse, como advérbios, adjetivos e etc\n",
        "for i in range(len(data)):\n",
        "  doc = data['textos'][i]\n",
        "  doc = tag(doc)\n",
        "  verbos_nomes = []\n",
        "  for token in doc:\n",
        "    if token.pos_ == 'VERB' or token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
        "        verbos_nomes.append(token.lemma_)\n",
        "\n",
        "  text = \" \".join(verbos_nomes)\n",
        "  data['textos'][i] = text"
      ],
      "metadata": {
        "id": "y8avRttQtaoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - qtd caracteres texto 1\n",
        "len(data['textos'][0])"
      ],
      "metadata": {
        "id": "uGNYVIqWtaql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## leitura dos metadados - arquivo excel\n",
        "## metadados obtidos no hotsite da Agenda 2030 no STF\n",
        "\n",
        "# pasta do google drive onde o arquivo xlsx se encontra\n",
        "%cd \"/content/drive/MyDrive/...\"\n",
        "\n",
        "# trocar nome do arquivo\n",
        "ODS = pd.read_excel(\"nome_arquivo_metadados.xlsx\") "
      ],
      "metadata": {
        "id": "NCOgyrPwtas6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto ODS\n",
        "print(np.unique(ODS['DESC_ODS']))"
      ],
      "metadata": {
        "id": "XYPX6FCAtavq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pivoteamento (objeto ODS é long) + limpeza colunas inúteis\n",
        "ODS_bin = ODS.drop(columns='DESC_ODS').join(pd.get_dummies(ODS['DESC_ODS']))\n",
        "\n",
        "# exemplo de limpeza - coluna inútil\n",
        "# repetir linha de código .drop para todas as colunas inúteis\n",
        "ODS_bin.drop(\"NUMERO\", inplace=True, axis=1)\n",
        "\n",
        "# groupby - ID\n",
        "ODS_bin = ODS_bin.groupby([\"ID\"]).sum()\n",
        "\n",
        "# rename - fazer isso para todos os ODS que aparecem na saída da célula anterior\n",
        "# os exemplos abaixo contemplam apenas alguns ODS\n",
        "ODS_bin.rename(columns={'ODS 10 - Redução das Desigualdades': 'ODS_10'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 11 - Cidades e Comunidades Sustentáveis': 'ODS_11'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 16 - Paz, Justiça e Instituições Eficazes': 'ODS_16'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 17 - Parcerias e Meios de Implementação': 'ODS_17'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 5 - Igualdade de Gênero': 'ODS_5'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 8 - Trabalho Decente e Crescimento Econômico': 'ODS_8'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 3 - Saúde e Bem-Estar': 'ODS_3'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 1 - Erradicação da Pobreza': 'ODS_1'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 12 - Consumo e Produção Responsáveis': 'ODS_12'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 15 - Vida Terrestre': 'ODS_15'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 2 - Fome Zero e Agricultura Sustentável': 'ODS_2'}, \n",
        "               inplace = True)\n",
        "ODS_bin.rename(columns={'ODS 4 - Educação de Qualidade': 'ODS_4'}, \n",
        "               inplace = True)"
      ],
      "metadata": {
        "id": "zqn2Y2RPtax6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto ODS_bin\n",
        "ODS_bin"
      ],
      "metadata": {
        "id": "t6uNrJNlta0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## criação do objeto data final - merge\n",
        "data_final = pd.merge(data,ODS_bin,on='ID',how='right')"
      ],
      "metadata": {
        "id": "rFt3KMData2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## lowercase novamente - PROPN spacy altera a letra inicial\n",
        "data_final['textos'] = [f.lower() for f in data_final['textos']]"
      ],
      "metadata": {
        "id": "WnMOFbYCD2Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto data_final\n",
        "data_final"
      ],
      "metadata": {
        "id": "qQstzSnmta43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### visualizações\n",
        "## criando uma cópia do objeto data_final\n",
        "data_final_viz = data_final.copy(deep=True)"
      ],
      "metadata": {
        "id": "IQkXy065ta7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## incluindo uma coluna com os tokens de cada texto\n",
        "data_final_viz['tokens_textos'] = data_final_viz.apply(lambda row: nltk.word_tokenize(row['textos']), axis=1)"
      ],
      "metadata": {
        "id": "gvicnpDWta96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto data_final_viz\n",
        "data_final_viz"
      ],
      "metadata": {
        "id": "aaPA9SfutbAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - tokens do texto 1\n",
        "data_final_viz[\"tokens_textos\"][0]"
      ],
      "metadata": {
        "id": "5vFT-DPwtbCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## frequência de palavras - texto 1\n",
        "contagem_palavras = collections.Counter(data_final_viz[\"tokens_textos\"][0])\n",
        "print(\"palavras mais comuns:\", contagem_palavras.most_common(10))"
      ],
      "metadata": {
        "id": "_1A2QI6XtbEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## transformação em data frame - frequência de palavras texto 1\n",
        "contagem_palavras = pd.DataFrame(contagem_palavras.most_common(25),\n",
        "                                     columns=[\"palavra\", \"freq\"])"
      ],
      "metadata": {
        "id": "SU3M6bZFtbHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto contagem_palavras\n",
        "print(contagem_palavras)"
      ],
      "metadata": {
        "id": "n2Y1HdyqtbJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## gráfico de palavras mais frequentes\n",
        "# set tamanho da figura\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "# horizontal bar plot\n",
        "contagem_palavras.sort_values(by='freq').plot.barh(x=\"palavra\", y=\"freq\", ax=ax)\n",
        "# título\n",
        "plt.title(\"palavras mais comuns no texto\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HYlGe0_UtbMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## nuvem de palavras\n",
        "word_cloud = WordCloud(collocations = False, background_color = 'white').generate(data_final_viz[\"textos\"][0])\n",
        "plt.figure(figsize=(16, 13), dpi = 400)\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PvkRw66MtbOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### contagem de palavras-chave\n",
        "## leitura do texto bruto\n",
        "textos_brutos = []\n",
        "for f in files:\n",
        "  with fitz.open(f) as doc:\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "  textos_brutos.append(text)"
      ],
      "metadata": {
        "id": "yswoFvxetbQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## limpeza mínima\n",
        "textos_brutos = [f.lower() for f in textos_brutos]\n",
        "textos_brutos = [f.strip() for f in textos_brutos]\n",
        "textos_brutos = [\" \".join(f.split()) for f in textos_brutos]"
      ],
      "metadata": {
        "id": "Jlz2pDsrtbS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto textos brutos\n",
        "textos_brutos"
      ],
      "metadata": {
        "id": "BWbSm6XytbVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## palavras-chave de exemplo\n",
        "palavras_chave_1 = [\"saúde\", \"bem-estar\", \"mortalidade materna\", \"mortalidade infantil\", \"mortalidade neonatal\", \"aids\", \"hiv\",\n",
        "                  \"tuberculose\", \"malária\", \"hepatite\", \"saúde mental\", \"câncer\", \"diabetes\", \"doenças respiratórias\"]\n",
        "palavras_chave_2 = [\"suicídio\", \"álcool\", \"drogas\", \"entorpecentes\", \"acidentes de trânsito\", \"saúde mental\", \"saúde reprodutiva\",\n",
        "                  \"saúde essencial\", \"saúde pública\", \"doença\", \"medicamentos\", \"vacina\",\"ministro\",\"ministro presidente\",\"suspensão\",\"plenário\",\"centro\"]\n",
        "palavras_chave = [palavras_chave_1,palavras_chave_2]"
      ],
      "metadata": {
        "id": "QlBGe4WOtbX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loop para contar ocorrências\n",
        "contagem = []\n",
        "for p in palavras_chave:\n",
        "  cont = [sum(f.count(x) for x in p) for f in textos_brutos]\n",
        "  contagem.append(cont)\n",
        "contagem = pd.DataFrame(contagem).transpose()\n",
        "contagem.columns = ['ODS_TESTE_1', 'ODS__TESTE_2']"
      ],
      "metadata": {
        "id": "DPArZc2PtbaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - objeto contagem\n",
        "contagem"
      ],
      "metadata": {
        "id": "n_WzsYzMtbch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### classificação via redes neurais\n",
        "## foco no ODS 16 - groupby \n",
        "data_final.groupby(['ODS_16']).size().plot.bar()"
      ],
      "metadata": {
        "id": "ComaRVPmtbfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## opcional - tamanho dos textos\n",
        "data_final['textos'] = data_final['textos'].astype(str)\n",
        "data_final['length'] = data_final.textos.str.len()\n",
        "data_final = data_final[data_final.length > 10]\n",
        "data_final.drop(\"length\", inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "R46LKA4-tbhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BERT\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)"
      ],
      "metadata": {
        "id": "eku4OV-etbkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## criação dos labels\n",
        "labels = {0:0,\n",
        "          1:1\n",
        "          }"
      ],
      "metadata": {
        "id": "CWvhAHbitbm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## classe Dataset - Pytorch\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_final):\n",
        "\n",
        "        self.labels = [labels[label] for label in data_final['ODS_16']]\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 32, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in data_final['textos']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y\n"
      ],
      "metadata": {
        "id": "BLi6qD-6tbpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## conjunto de treino, teste e validação - 25, 35 e 40 são limites de exemplo\n",
        "## a montagem das bases de treino, validação e testes pode ser feita de forma \n",
        "## aleatório também. \n",
        "df_train = data_final.iloc[0:25]\n",
        "df_val = data_final.iloc[25:35]\n",
        "df_test = data_final.iloc[35:40]"
      ],
      "metadata": {
        "id": "HSzk3sO4tbrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## verificação - tamanho dos conjuntos\n",
        "print(len(df_train),len(df_val), len(df_test))"
      ],
      "metadata": {
        "id": "OXjj38zqtbuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## classificador\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.4):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "GCxTSsrttbw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## execução da rede\n",
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "                batch_loss = criterion(output, train_label)\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label)\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "                  \n",
        "EPOCHS = 10\n",
        "model = BertClassifier()\n",
        "LR = 1e-6\n",
        "              \n",
        "train(model, df_train, df_val, LR, EPOCHS)"
      ],
      "metadata": {
        "id": "ZDqL4Vwvtbzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## avaliação da rede\n",
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    global total_acc_test\n",
        "    total_acc_test = 0\n",
        "    global lista\n",
        "    lista = []\n",
        "    global lista_pred\n",
        "    lista_pred = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "              \n",
        "              lista_pred.append(output.argmax(dim=1))\n",
        "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "              lista.append(acc)\n",
        "              total_acc_test += acc\n",
        "    \n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "    \n",
        "    \n",
        "evaluate(model, df_test)"
      ],
      "metadata": {
        "id": "KrgobzHstb2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## acertos da rede\n",
        "total_acc_test"
      ],
      "metadata": {
        "id": "l7D-lOJItb4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## valores preditos\n",
        "lista_pred = [f.item() for f in lista_pred]"
      ],
      "metadata": {
        "id": "Kjjo1eu4tb6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### classificação via XGboost\n",
        "## bag of words\n",
        "vectorizer = CountVectorizer(decode_error='replace', encoding='utf-8')\n",
        "bow_transformer_train = vectorizer.fit(df_train[\"textos\"].values.astype('U'))\n",
        "bow_transformer_test = vectorizer.fit(df_test[\"textos\"].values.astype('U'))\n",
        "bow_transformer_val = vectorizer.fit(df_val[\"textos\"].values.astype('U'))"
      ],
      "metadata": {
        "id": "vSYoXWLitb88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## convertendo strings em vetores usando BoW\n",
        "textos_bow_train = bow_transformer_train.transform(df_train[\"textos\"].values.astype('U'))\n",
        "textos_bow_test = bow_transformer_test.transform(df_test[\"textos\"].values.astype('U'))\n",
        "textos_bow_val = bow_transformer_val.transform(df_val[\"textos\"].values.astype('U'))"
      ],
      "metadata": {
        "id": "mChWh1rfF2T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## shape textos_bow\n",
        "print(f\"Shape: {textos_bow_train.shape}\")\n",
        "print(f\"Quantidade de ocorrências diferentes de zero: {textos_bow_train.nnz}\")"
      ],
      "metadata": {
        "id": "Hlu1tLOaF2WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TF-IDF\n",
        "tfidf_transformer_train = TfidfTransformer().fit(textos_bow_train)\n",
        "textos_tfidf_train = tfidf_transformer_train.transform(textos_bow_train)\n",
        "\n",
        "tfidf_transformer_test = TfidfTransformer().fit(textos_bow_test)\n",
        "textos_tfidf_test = tfidf_transformer_test.transform(textos_bow_test)\n",
        "\n",
        "tfidf_transformer_val = TfidfTransformer().fit(textos_bow_val)\n",
        "textos_tfidf_val = tfidf_transformer_val.transform(textos_bow_val)"
      ],
      "metadata": {
        "id": "d33GbaZpF2Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## shape textos_tfidf\n",
        "print(textos_tfidf_train.shape)"
      ],
      "metadata": {
        "id": "PhrHTgfvF2a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## split textos_tfidf em conjuntos de treino e testes\n",
        "label_train = df_train['ODS_16']\n",
        "label_val = df_val['ODS_16']\n",
        "label_test = df_test['ODS_16']"
      ],
      "metadata": {
        "id": "5fw0hMlZF2dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## visualização - objeto label_train\n",
        "label_train"
      ],
      "metadata": {
        "id": "ljdQU4JIF2fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## instância do modelo\n",
        "clf = XGBClassifier()\n",
        "clf.fit(textos_tfidf_train, label_train)"
      ],
      "metadata": {
        "id": "hf2GMES5F2hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## predicões - conjunto de treino\n",
        "predict_train = clf.predict(textos_tfidf_train)\n",
        "\n",
        "print(\n",
        "    f\"Acurácia - Conjunto de Treino: {metrics.accuracy_score(label_train, predict_train):0.3f}\"\n",
        ")"
      ],
      "metadata": {
        "id": "1nIELyVcF2j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## exemplo de predição - texto específico\n",
        "print(\n",
        "    \"predição:\",\n",
        "    clf.predict(\n",
        "        tfidf_transformer_train.transform(bow_transformer_train.transform([data_final[\"textos\"][9]]))\n",
        "    )[0],\n",
        ")\n",
        "print(\"classe verdadeira:\", data_final[\"ODS_16\"][9])"
      ],
      "metadata": {
        "id": "Tlebm9Lvtb_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## acurácia - conjunto de teste\n",
        "label_predictions = clf.predict(textos_tfidf_test)\n",
        "print(f\"Acurácia do modelo: {metrics.accuracy_score(label_test, label_predictions):0.3f}\")"
      ],
      "metadata": {
        "id": "WA2EKlQbGOhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## examinando o objeto - label_predictions\n",
        "label_predictions"
      ],
      "metadata": {
        "id": "UsrpPGgFGOqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### similaridade entre textos\n",
        "## TF IDF - Embedding\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=50, max_df=0.50,\n",
        "                             stop_words=stopwords.words('portuguese'), \n",
        "                             analyzer='word',\n",
        "                             ngram_range=(1, 1), \n",
        "                             lowercase=True, \n",
        "                             use_idf=True)\n",
        "X = vectorizer.fit_transform(data_final['textos']).toarray()"
      ],
      "metadata": {
        "id": "P_ET6UqPGOs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## similaridade por cosseno usando TF IDF\n",
        "idx_1 = data_final.index\n",
        "idx_2 = data_final.index\n",
        "simil = 1 - scipy_cdist(X[idx_2], \n",
        "                               X[idx_1], 'cosine')\n",
        "simil = np.around(simil, decimals=3)\n",
        "\n",
        "cos_sims_df = pd.DataFrame(data=simil,\n",
        "                       columns=data_final.ID.loc[idx_1].tolist(),\n",
        "                       index=data_final.ID.loc[idx_2].tolist())\n",
        "cos_sims_df.sort_index(axis=1, inplace=True) "
      ],
      "metadata": {
        "id": "Gqswqz3VGOvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## avaliação - objeto cos_sims_df\n",
        "cos_sims_df"
      ],
      "metadata": {
        "id": "gGaXf-zCGOxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mx-jFpL0GOzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P__EOQJLGO2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N7U1D0xZGO4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0-4izI-ZGO6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5MOMr2r2GO85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B5gBs4VnGO--"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}