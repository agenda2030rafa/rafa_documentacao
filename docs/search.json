[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "DocumentaçãoRAFA 2030\n\n1 Introdução\nA iniciativa RAFA 2030 é parte do projeto Agenda 2030 no STF, idealizado pela gestão do Ministro Luiz Fux. O objetivo maior do projeto é internalizar a Agenda 2030 da ONU no dia a dia da corte, de forma a torná-la uma referência prática para decisões em nível tático e estratégico. A Agenda 2030 busca melhorar a vida de todos os habitantes do planeta e é resultado de amplas discussões entre os países membros da ONU. Etiquetar processos judiciais de acordo com Objetivos de Desenvolvimento Sustentável (ODS) da Agenda 2030 da ONU é um importante passo para incluir a própria Agenda no cotidiano do tribunal, uma vez que tais ODS são citados em votos dos Ministros e podem, no futuro, gerar precedência para temas de maior impacto social. Os gestores do projeto Agenda 2030 no STF, então, perceberam que o desenvolvimento de uma ferramenta computacional para auxílio na classificação de processos em ODS seria de grande utilidade para os setores que tratam da autuação e acervo do tribunal, pois tal tarefa é realizada de forma manual atualmente. A iniciativa RAFA surgiu neste contexto, em outubro de 2020, inicialmente como um contador de palavras-chave em processos candidatos a receber etiquetas de ODS da Agenda 2030 da ONU.\nAo longo do tempo, a iniciativa ganhou corpo e cresceu em funcionalidades. Atualmente, a RAFA se concentra em duas tarefas específicas relacionadas à classificação de processos em ODS da Agenda 2030: classificação automática de grandes conjuntos (lotes) de processos judiciais via aprendizagem de máquina e deep learning e apoio visual e estatístico para classificação individual de processos judiciais. Ambas estão em período de testes atualmente e contam com a curadoria de servidores da área jurídica. A primeira tarefa consiste em utilizar algoritmos de aprendizagem de máquina e deep learning para classificar automaticamente lotes de processos judiciais em ODS da Agenda 2030. O fluxo em si é bastante simples. A área jurídica do tribunal indica conjuntos de peças jurídicas - que podem ser os processos autuados mais recentemente, por exemplo - e a RAFA utiliza textos já etiquetados para treinar e etiquetar as novas entradas. A entrega, neste caso, são as classificações que resultam dos algoritmos ajustados. A segunda tarefa é centrada no desenvolvimento de um aplicativo para apoiar a decisão de classificar uma nova entrada (processo) em ODS da Agenda 2030. Este app conta com diversos gráficos utilizados em processamento de linguagem natural (NLP), além de contagem de palavras-chave e busca de determinadas palavras e leis no contexto da peça jurídica. A entrega, neste caso, é a possibilidade de ajudar o servidor a classificar mais processos jurídicos em menos tempo e com maior qualidade.\nApresentar os aspectos técnicos da RAFA em suas duas frentes (classificação automática e app) é o objetivo desta documentação, que não se dedica a apresentar apenas as ideias que hoje compõem a ferramenta. Todo o fluxo de desenvolvimento da RAFA será apresentado, incluindo as ideias que não se mostraram as melhores em termos de performance e sequer entraram nos módulos que hoje estão em teste. De maneira mais clara, todas as estratégias e algoritmos utilizados serão apresentadas na documentação, alguns com extratos de código e outros apenas com registros e referências de implementação. O objetivo não é tornar a documentação excessivamente longa, mas ajudar de maneira geral. O que deu errado para RAFA pode ser decisivo na resolução de problemas de NLP em outros tribunais. Um projeto em linguagem R e um notebook em Python serão disponibilizados com referências, gráficos, códigos e exemplos de imediata aplicação. Críticas à iniciativa e à organização desta documentação são bem vindas e devem ser enviadas para sge@stf.jus.br. A presente documentação não é estática e será necessariamente aumentada e revisada conforme novos módulos da RAFA sejam feitos."
  },
  {
    "objectID": "cap2.html",
    "href": "cap2.html",
    "title": "2  Ambiente de desenvolvimento",
    "section": "",
    "text": "A iniciativa RAFA foi desenvolvida em linguagem R e Python, prioritariamente. Isso significa que, com exceção de alguns poucos passos intermediários, tudo aquilo que será apresentado nesta documentação envolve apoio computacional de alguma linguagem de programação (R ou Python, neste caso). Neste capítulo, serão apresentadas informações sobre a montagem do ambiente, características das máquinas utilizadas no desenvolvimento e versões de softwares e bibliotecas utilizadas, bem como o registro de algumas referências importantes sobre a etapa de conectar as linguagens R e Python.\nO time da iniciativa tem integrantes da área jurídica e do escritório de estatística do tribunal, estes últimos responsáveis pelo desenvolvimento técnico da ferramenta. Os membros da área jurídica são responsáveis pela curadoria dos resultados e pela gestão do projeto Agenda 2030 como um todo. O corpo técnico escolheu usar R e Python pois são as linguagens mais populares para ciência de dados, mas os registros desta documentação podem ser reproduzidos em outras linguagens de programação com algum esforço de código e pesquisa de pacotes/bibliotecas. Outras opções populares são: Scala, para utilização do Ecossistema Hadoop, Julia, Go e C++.\nDado que Python e R são linguagens populares em ciência de dados, não é difícil encontrar informações e cursos sobre processamento de linguagem natural (NLP) na internet. Reproduzir códigos diretamente de fontes não oficiais, no entanto, pode ser complicado e até limitar o potencial de desenvolvimento. Neste contexto, um possível caminho envolve pesquisar algoritmos e estratégias para resolver a tarefa de interesse em boas plataformas (Towards Data Science e Towards AI do Medium, Kaggle e MachineHack) e partir para análise da documentação oficial dos frameworks/pacotes utilizados, como Tensorflow, Keras e Pytorch em Python ou Quanteda e Text em R. Evidentemente, tais pesquisas não excluem a necessidade de formação. Apesar de todos os membros do escritório de estatística envolvidos na iniciativa possuírem graduação/pós graduação em estatística, houve necessidade de complementar a formação superior com cursos especializados em aprendizagem de máquina, deep learning e inteligência artificial. A estrutura da formação complementar pode ser bootcamp, pós graduação ou até mesmo cursos livres, mas é importante que o conteúdo cubra todo o fluxo de ciência de dados, a saber: entrada de dados – manipulação – visualização – modelos – deploy – apresentação. Nas referências desta documentação existem sugestões de plataformas e cursos.\nA infraestrutura necessária para desenvolvimento de aplicações com deep learning pode ser um entrave, dado que as máquinas precisam de componentes robustos e, em alguns casos, de placas de vídeo dedicadas. Por este motivo, a presente documentação apresentará as soluções em R e Python, sendo que para a última linguagem é possível utilizar o Google Colab1, que é um serviço de nuvem gratuito hospedado pelo próprio Google centrado em facilitar desenvolvimento de aprendizagem de máquina e inteligência artificial. Trata-se de uma ferramenta que permite misturar código e texto em uma técnica conhecida como notebook. Na prática, o Google Colab permite rodar códigos em Python necessitando apenas de uma conta Google e isso reduz significativamente os problemas com ambiente, dado que o próprio Colab tem vários pacotes/bibliotecas em Python instalados nativamente. A instalação local do Python será feita através da distribuição Anaconda e servirá apenas para conectar Python e R, com objetivo de rodar Tensorflow e Keras através da IDE RStudio tal qual indica a referência Hvitfeldt e Silge (2021)2. A instalação local da linguagem R é simples, com exceção do passo de conexão entre R e Python, que envolve um pouco mais de atenção e será melhor explicado adiante no capítulo.\nInicialmente, uma máquina pessoal de um membro da iniciativa foi utilizada para desenvolvimento. Trata-se de um notebook com 32gb de RAM, 2T de SSD e processador Intel i9, com placa de vídeo Geforce GTX 1660 Ti. Em um segundo momento, tal notebook foi substituído por uma máquina virtual com os mesmos 32gb de RAM e processador Xeon. Trata-se de uma máquina instanciada apenas para desenvolvimento da ferramenta, com poucos programas instalados e completamente dedicada para processamento dos textos. A seguir serão apresentados aspectos técnicos da instalação local da linguagem R e as versões utilizadas para softwares e pacotes. A documentação seguirá, neste capítulo, com a instalação da distribuição Anaconda Python, conexão Python e R e a apresentação da organização geral do projeto em linguagem R e dos notebooks em Python."
  },
  {
    "objectID": "cap2.html#instalação-local-da-linguagem-r",
    "href": "cap2.html#instalação-local-da-linguagem-r",
    "title": "2  Ambiente de desenvolvimento",
    "section": "2.1 Instalação local da Linguagem R",
    "text": "2.1 Instalação local da Linguagem R\nA instalação local da Linguagem R não envolve muitas dificuldades e pode ser feita através das orientações dadas na página oficial da linguagem. Recomenda-se fortemente o uso da IDE RStudio para desenvolvimento em R. Trata-se de um ambiente integrado de desenvolvimento onde é possível fazer a gestão dos pacotes necessários, upload de bases de dados sem necessidade de usar linha de código e avaliação das saídas de visualização. Também é possível conectar o RStudio ao GitHub, o que permite melhor versionamento dos códigos. A interface da IDE RStudio pode ser vista na Figura 2.1:\n\n\n\nFigura 2.1: Interface RStudio\n\n\nDentro da IDE RStudio é possível montar projetos em R. Projetos são, em última análise, propostas de organização para códigos em R. Com projetos, pode-se organizar as bases e os scripts em pastas diferentes, bem como incluir documentações e manuais em PDF. Uma das maiores vantagens, para além da organização em si, é a montagem do ambiente. Usando projetos, o ambiente de desenvolvimento fica explicitamente definido como uma pasta do próprio projeto, o que facilita, por exemplo, o uso de caminhos para localizar arquivos específicos. A organização geral do projeto em R da iniciativa RAFA será melhor apresentada em seções subsequentes."
  },
  {
    "objectID": "cap2.html#sec-22",
    "href": "cap2.html#sec-22",
    "title": "2  Ambiente de desenvolvimento",
    "section": "2.2 Instalação local da distribuição Anaconda Python e conexão com R",
    "text": "2.2 Instalação local da distribuição Anaconda Python e conexão com R\nPara ajustar redes neurais em R, uma alternativa possível é conectar a Linguagem Python e assim utilizar os frameworks Keras e Tensorflow, específicos para deep learning. Isso certamente não é simples e existem outros pacotes em R capazes de executar a mesma tarefa, porém não com a mesma performance. O fluxo é simples: Instala-se a distribuição local Anaconda Python3 e em seguida, as bibliotecas Keras e Tensorflow. Na sequência, instalam-se alguns pacotes em linguagem R para fazer tal conexão, sendo que um ambiente dedicado dentro do Anaconda se faz necessário. A conexão entre R e Python pode ser feita com ajuda das orientações disponíveis neste link, mas há um script chamado python_r na pasta zip contendo um passo a passo. Para verificar se a conexão entre R e Python está funcionando corretamente, pode-se chamar uma base de dados interna ao Tensorflow (MNIST), como mostra o código a seguir:\n\n# Carregar pacotes\nlibrary(keras)\nlibrary(tensorflow)\n\n# Exemplo - Dataset MNIST\nmnist <-  dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nA biblioteca Tensorflow é uma das ferramentas que compõem o estado da arte em deep learning, mas seu uso não é simples. O pacote Keras surgiu com o objetivo de facilitar o uso do Tensorflow e por isso essa associação (Keras + Tensorflow) é tão usada em projetos de aprendizagem de máquina e inteligência artificial. Com os pacotes em Linguagem R, há ainda outro benefício: as redes e todas as funções essenciais para avaliação de performance e tratamento inicial dos dados podem ser feitas, com Keras e Tensorflow, em sintaxe R e tidy, como mostra a uma rede em Keras no R:\n\n# Primeira rede neural\ndense_model <- keras_model_sequential() %>% \n  layer_embedding(input_dim = max_words + 1,\n                  output_dim = 32,\n                  input_length = max_length) %>% \n  layer_global_average_pooling_1d() %>% \n  layer_dense(units = 32, activation = \"linear\") %>% \n  layer_dense(units = 32, activation = \"sigmoid\")\n\ndense_model\n\nSintaxe tidy é o termo utilizado pela comunidade da Linguagem R para os códigos escritos com apoio dos pacotes tidyverse e dplyr, que facilitam enormemente a manipulação de dados. Com os pacotes deste universo (tidy), as operações características de data wrangling tais como select, slice, distinct, subset, mutate e rename são feitas de maneira intuitiva e simples, com uso de um operador chamado pipe (%\\>%). Abaixo apresenta-se exemplos de operações realizadas com o pacote dplyr:\n\n# Dividindo por sentenças - Exemplo: Primeiro texto\ntexto_sentencas <- tibble(text = dados$texto[1]) %>% \n  unnest_tokens(sentenca, text, token = \"sentences\") %>% \n  mutate(sentenca_id = row_number()) %>% \n  select(sentenca_id, sentenca)"
  },
  {
    "objectID": "cap2.html#organização-geral-do-projeto-em-linguagem-r-e-dos-notebooks-python",
    "href": "cap2.html#organização-geral-do-projeto-em-linguagem-r-e-dos-notebooks-python",
    "title": "2  Ambiente de desenvolvimento",
    "section": "2.3 Organização geral do projeto em linguagem R e dos notebooks Python",
    "text": "2.3 Organização geral do projeto em linguagem R e dos notebooks Python\nO projeto em linguagem R foi estruturado pensando na utilização encadeada dos scripts e em boas práticas para organização e versionamento de código. Basicamente, a raiz do projeto contém as pastas principais, a saber: dados, imagens e scripts. Contém, também, um documento simples com informações básicas de hardware, versões dos softwares utilizados e montagem do ambiente. A pasta raiz do projeto pode ser vista na Figura 2.2:\n\n\n\nFigura 2.2: Pasta raiz do projeto\n\n\nA pasta scripts, como se pode imaginar, contém extratos de códigos da iniciativa, além de um catálogo. O objetivo de catalogar os scripts é sugerir uma ordem de utilização e explicar, em linhas gerais, a tarefa a ser realizada por cada um. Os scripts são, em ordem alfabética: limpeza, pacotes_funcoes, redes_neurais. Todos estão devidamente comentados. A pasta dados contém as bases brutas e manipuladas em linguagem R (xlsx e csv), além dos arquivos em PDF para a leitura, insumo da iniciativa. Tal como na pasta scripts, existe um catálogo de tabelas, para que o usuário identifique tabelas brutas e manipuladas, bem como o conteúdo de cada um dos registros. A organização da pasta pode ser vista na Figura 2.3:\n\n\n\nFigura 2.3: Organização pasta dados\n\n\nPode-se notar que existem ainda duas pastas dentro da pasta dados: Petições e Acórdãos. Tais pastas organizam os arquivos em PDF e os dividem entre petições iniciais e acórdãos, os dois tipos de documentos jurídicos que são utilizados na iniciativa RAFA. Por questões de infraestrutura e base de dados, os documentos possuem, originalmente, nomes com estrutura do tipo classe_numero_peticao_inicial_xxx.pdf ou classe_numero_acordao_xxx.pdf, como a Figura 2.4 mostra:\n\n\n\nFigura 2.4: Nomes - peças\n\n\nNeste caso, a própria linguagem R associada à regex (expressões regulares) pode separar as petições e os acórdãos, caso os documentos estejam todos em uma mesma pasta. O script fluxo serve ao propósito de apresentar um exemplo de movimentação de arquivos em pastas usando linguagem R, via regex. Trata-se de uma automatização útil, que pode render a economia de algumas horas caso a base de arquivos seja grande.\nPor fim, o projeto R contém o arquivo do projeto em si (aquele que abre o projeto na IDE) e uma pasta de imagens, que pode ser utilizada pelo usuário para guardar imagens e outputs diversos. Além do projeto em R, que contém alguns dos módulos em teste da iniciativa RAFA, também serão disponibilizados scripts isolados para operações e tarefas específicas, tais como clusterização e similaridade entre textos, modelos outros de aprendizagem de máquina, pos-tag e análise sintática de sentenças. Tais scripts serão referenciados no corpo da presente documentação e serão organizados numa seção anexa ao texto. A intenção de disponibilizar exemplos de abordagens e ideias que não compõem oficialmente a iniciativa é clara e objetiva: o que não se mostrou bom para a RAFA pode resolver outro problema.\nOs notebooks em Python foram reduzidos a um só, que realiza as mesmas tarefas dos scripts em R. Neste caso, recomenda-se a utilização do Google Colab Pro, que disponibiliza RAM alta e TPUs para deep learning. A única diferença entre os scripts em R e Python se dá pela utilização do Pytorch. Enquanto em R a opção foi usar Tensorflow e Keras, em Python existem códigos em Pytorch. Isso torna a iniciativa mais geral e mais ampla, além de permitir a utilização de modelos mais avançados como o BERT e outros tipos de transformers. Como nos scripts em R, os códigos do notebook em Python estão comentados, como se pode ver na Figura 2.5 a seguir:\n\n\n\nFigura 2.5: Notebook Python\n\n\nEm resumo, serão entregues junto da documentação um projeto em R, um notebook em Python e um arquivo zip contendo scripts individuais para tarefas específicas, com códigos em R e Python. A documentação segue com as informações sobre entrada, leitura e limpeza dos dados, que é uma etapa importante da iniciativa. Neste contexto, são necessárias algumas explicações sobre as atividades de OCR (ocerização) e pos-tag."
  },
  {
    "objectID": "cap3.html",
    "href": "cap3.html",
    "title": "3  Entrada, leitura e limpeza dos dados em PDF",
    "section": "",
    "text": "Os principais dados de entrada da RAFA são textos de petições iniciais e acórdãos de processos do STF. Tratam-se de arquivos em formato .PDF que podem ser nato digitais ou imagens. Imagens são típicas em processos digitalizados e exigem um passo de OCR para obtenção do texto. Um exemplo de processo que precisa de OCR é dado pela Figura 3.1:\nNos casos onde o PDF contém fotos, imagens digitalizadas, manchas ou bordas é necessário utilizar pacotes de ocerização. Em linhas gerais, a atividade de OCR nada mais é do que extrair texto de imagens. Entre as iniciativas mais populares de OCR, destaca-se o Tesseract. Trata-se de um projeto desenvolvido em C e C++ pela empresa HP e mantido temporariamente pelo Google, que está atualmente disponível no Github. Reúne redes neurais de arquiteturas LSTM com foco em reconhecimento de caracteres de texto em imagens de extensão .PNG, .JPEG e .TIFF. Pode ser utilizado em Python e R, como o script tesseract_pt_R mostra. Como exemplo, pode-se avaliar a imagem a seguir:\nTrata-se de uma carta de aceitação em um congresso. O PDF, que será disponibilizado no arquivo zip, não permite seleção do texto. É apenas uma imagem digitalizada. Ao executar parte dos códigos que estão presentes no script tesseract_pt_R temos o seguinte objeto em R:\nComo existem manchas no papel e sombras decorrentes da iluminação local, alguns caracteres não presentes no texto acabam surgindo na ocerização. Isso é normal. O objeto em R, no entanto, deixa claro que, assim como em um texto nato digital, o conjunto de caracteres obtido via OCR tem potencial para se tornar claro e totalmente legível por máquina, depois de alguns passos de limpeza. Para avaliar o desempenho do Tesseract em textos jurídicos, a figura seguinte mostra o texto resultante de OCR no processo ADPF 56 (referenciado na Figura 3.4):\nQue com algum esforço de limpeza, como retirada de espaços em branco desnecessários e remoção de caracteres especiais e inúteis, resulta no seguinte texto limpo:\nQuando o arquivo em PDF é nato digital, o processo de leitura se torna mais fácil e rápido. Em linguagem R, por exemplo, existe o pacote pdftools, que executa muito bem a leitura. Em Python, é possível usar os pacotes PyMuPDF e PyPDF2, ambos de fácil instalação no ambiente virtual do Google Colab. Quando não há necessidade de ocerização, é possível fazer a leitura de muitos processos em poucos segundos.\nQuando os processos de entrada são lidos e seus textos estão contidos em objetos da linguagem R e Python, é possível iniciar a limpeza dos textos. A próxima seção se ocupará desta fundamental tarefa em processamento de linguagem natural."
  },
  {
    "objectID": "cap3.html#limpeza-dos-textos-jurídicos",
    "href": "cap3.html#limpeza-dos-textos-jurídicos",
    "title": "3  Entrada, leitura e limpeza dos dados em PDF",
    "section": "3.1 Limpeza dos textos jurídicos",
    "text": "3.1 Limpeza dos textos jurídicos\nA limpeza dos textos é parte fundamental em qualquer fluxo de processamento de linguagem natural (NLP) e para textos técnicos adquire ainda maior importância. Dado que o processamento de textos por máquinas normalmente utiliza palavras como unidade básica, a etapa de limpeza serve ao propósito de deixar o texto mais denso e livre de palavras desnecessárias para o entendimento do objeto central, chamadas stopwords ou palavras de parada. Caracteres especiais como marcadores de pontuação, por exemplo, podem tornar os textos difíceis de processar e por isso também devem ser removidos. Computadores lidam melhor com números do que com strings e a etapa de limpeza também serve ao objetivo de tornar os textos menores, melhorando, assim, a capacidade de processamento dos modelos de NLP.\nA natureza dos textos alvo de limpeza faz toda diferença na própria limpeza, dado que as próprias stopwords podem mudar de área para área. Palavras de parada jurídicas, por exemplo, em nada se parecem com palavras inúteis de outros ramos, da mesma maneira que, em textos literários, a limpeza se torna ainda mais complicada, dado que os autores usam recursos de estilo não necessários em textos técnicos. Desta forma, é importante frisar que cada problema de NLP exige uma limpeza de textos específica. Em programação, tarefas muito específicas normalmente exigem emprego simultâneo de pacotes e funções customizadas. Pacotes são amplos conjuntos de funções, e no caso de NLP, devem ser complementados com funções criadas especificamente para o problema em tela, como no caso da limpeza de textos. Em linguagem R, há um enorme ecossistema de pacotes para manipulação/limpeza de textos, como o stringr, stringi, família de pacotes quanteda, text, tidytext, udpipe, entre outros. Na iniciativa RAFA, especificamente no projeto R, o script funcoes contém ferramentas customizadas de limpeza e organização dos textos e o script limpeza executa, efetivamente, a tarefa de limpar as peças jurídicas. Em Python, a etapa de limpeza é feita ao longo do notebook, através de list comprehensions e manipulação de dicionários/sets. Em ambas as linguagens, os códigos de limpeza de textos fazem uso intenso de regex (expressões regulares) e pos tag, que por ser uma atividade dotada de significado próprio em NLP, será melhor explicada em seções subsequentes.\nA etapa de limpeza dos textos é tão importante que se converte em uma feature dos problemas em si, em alguns fluxos de NLP. Isso significa que, dentro do mesmo problema, a limpeza pode mudar de acordo com a tarefa específica a ser realizada. Na iniciativa RAFA existem limpezas diferentes para cada algoritmo/tarefa, resumidas na tabela abaixo:\n\n\n\n\nTabela 1 – Limpeza x Algoritmos/Tarefas\n \n  \n    Limpeza \n    Tarefa \n  \n \n\n  \n    Texto bruto – sem limpeza \n    Contagem de palavras \n  \n  \n    Texto limpo – remoção stopwords e caracteres especiais \n    Algoritmos da família SVM e Naive Bayes, similaridade e clusterização \n  \n  \n    Texto limpo – remoção stopwords e caracteres especiais associada à atividade de pos tag \n    Redes neurais e algoritmos CatBoost \n  \n\n\n\n\n\n\nUm exemplo de limpeza pode ser visto na figura a seguir, onde se nota que o texto se torna menor e mais denso:\n\n\n\nFigura 3.6: Texto limpo"
  },
  {
    "objectID": "cap3.html#tarefa-pos-tag",
    "href": "cap3.html#tarefa-pos-tag",
    "title": "3  Entrada, leitura e limpeza dos dados em PDF",
    "section": "3.2 Tarefa Pos Tag",
    "text": "3.2 Tarefa Pos Tag\nA atividade de pos tag, do inglês part of speech tagging, é uma espécie de classificação gramatical de cada palavra em um texto. Trata-se de uma tarefa que pode ser intermediária, como no caso da iniciativa RAFA, ou a própria tarefa de interesse em fluxos de NLP. As etiquetas (tags) mais comuns na maioria dos idiomas são ADJ (adjetivo), ADV (advérbio), NOUN (substantivos), PROPN (nomes próprios) e VERB (verbos). Uma lista exaustiva das tags disponíveis no pacote spacy da linguagem Python está disponível neste link. A figura a seguir mostra um ajuste pos tag feito em linguagem R na frase “Inteligência artificial do STF aplicada à Agenda 2030”:\n\n\n\nFigura 3.7: Pos Tag em Linguagem R\n\n\nPode-se notar que a atividade pos tag etiqueta corretamente os termos da frase e há imenso potencial de utilização de tais etiquetas em outras tarefas de NLP, especificamente em limpeza de textos. Considerando que os textos devem ser densos e que remover partes desnecessárias é fundamental para diminuir a complexidade computacional do processamento, é razoável pensar em uma camada de pos tag posterior à remoção de caracteres especiais, mantendo apenas palavras com tags de verbo e nomes, por exemplo. A Figura 3.8 mostra exemplos do tamanho de um texto bruto e seu correspondente limpo com pos tag em linguagem Python:\n\n\n\nFigura 3.8: Tamanho em caracteres – Texto limpo e com pos tag\n\n\nImportante notar a significativa diferença em número de caracteres. Isso pode ser decisivo para melhorar a etapa de pré-processamento dos textos e embedding, tarefa que será melhor explicada em seções subsequentes. Utilizando linguagem R, é possível fazer pos tag em português com o pacote udpipe1, desde que se faça o download do modelo “portuguese-gsd”. Este extrato de código está disponível no arquivo zip. Em Python, é possível associar o pacote spacy e o pipeline pt_core_new_lg disponível no repositório HuggingFace2. Os comandos para desempenhar esta atividade em Python estão no notebook de referência."
  },
  {
    "objectID": "cap4.html",
    "href": "cap4.html",
    "title": "4  Metadados e manipulação do painel Agenda 2030 no STF",
    "section": "",
    "text": "Figura 4.1: Painel - Hotsite\n\n\nAtravés do botão .xlsx é possível baixar uma planilha contendo todos os processos, suas respectivas etiquetas de ODS e metadados. Tal planilha é utilizada no script limpeza do projeto em R e, depois de pivoteada, possui 18 colunas, sendo uma para o ID do processo e uma coluna binária para cada um dos 17 ODS da Agenda 2030. O objetivo, ao final da leitura e limpeza dos PDFs - passo que foi apresentado em seções anteriores - é usar comandos de natureza join do pacote dplyr para montar uma base com todos os textos dos processos e as respectivas marcações de ODS. Da tabela oriunda do hotsite ainda é possível usar outros metadados, tal como a classe processual, que pode ser uma das variáveis categóricas no algoritmo CatBoost. Tal algoritmo, cuja possível implementação está contida no notebook Python, é uma das opções. De posse das etiquetas e metadados, é possível apresentar as abordagens e estratégias de classificação, objetivo do capítulo seguinte.\n\n\n\n\n\nHotsite disponível em: https://portal.stf.jus.br/hotsites/agenda-2030/.↩︎"
  },
  {
    "objectID": "cap5.html",
    "href": "cap5.html",
    "title": "5  Abordagens e estratégias de classificação",
    "section": "",
    "text": "A primeira estratégia utilizada para classificação foi multilabel. Isso significa que cada processo avaliado pode ter todas as etiquetas (17 ODS) ou nenhuma delas e este tipo de classificação necessita de muitos dados. Considerando cada combinação possível de etiquetas como um rótulo individual, teríamos cerca de 131.071 etiquetas possíveis para menos de 3 mil processos avaliados pelos especialistas do tribunal, ou seja, teríamos mais categorias do que dados e isso é um enorme problema para modelos de previsão que envolvem treinamento e aprendizagem. Uma alternativa elegante para contornar este problema é dada pela utilização de modelos com probabilidade de pertencimento. Modelos deste tipo, após o treinamento, devolvem não uma coleção de etiquetas, mas uma probabilidade de cada etiqueta ser utilizada no registro que se quer prever a classificação.\nPara tal tarefa, uma possibilidade é utilizar o framework StarSpace do Facebook, cuja implementação em R pode ser encontrada no pacote ruimtehol. O StarSpace é, fundamentalmente, uma máquina de gerar embeddings. Embeddings são representações vetoriais de objetos quaisquer em espaços (vetoriais) dotados de distância. No caso das palavras, bons embeddings possuem vetores parecidos para palavras de significado próximo em linguagem natural, considerando o contexto de utilização. A implementação em R do StarSpace inclui algoritmos de similaridade e redes neurais, que, no caso de problemas multilabel, geram saídas com probabilidade de pertencimento. Teoricamente, o StarSpace mostra as n etiquetas mais prováveis para um determinado texto, sendo n um número arbitrário contido no intervalo entre 1 e o total de etiquetas. Neste caso, 17. Há um extrato de código no arquivo zip chamado starspace, com um exemplo de utilização da abordagem. A abordagem até gerou bons resultados, mas esbarrou na limitação dos próprios dados e apresentou um enorme problema no ODS 16. Apesar do StarSpace produzir embeddings robustos, classificar com um único algoritmo todos os ODS se mostrou uma tarefa muito pesada para o método, principalmente nos objetivos mais gerais, como no caso do décimo sexto, relacionado à justiça, paz e instituições eficazes. Maiores informações sobre o framework podem ser encontradas em neste link."
  },
  {
    "objectID": "cap5.html#similaridade",
    "href": "cap5.html#similaridade",
    "title": "5  Abordagens e estratégias de classificação",
    "section": "5.2 Similaridade",
    "text": "5.2 Similaridade\nA consequência natural da utilização do framework StarSpace é um estudo sobre similaridade de processos judiciais. Algumas iniciativas do judiciário utilizam similaridade para agrupar processos parecidos com diferentes propósitos, a exemplo da Berna (TJGO) e Athos (STJ). A iniciativa se ocupou de empregar similaridade com um objetivo de classificação posterior. Na prática, a RAFA calculava a similaridade entre um novo processo candidato a receber etiquetas de ODS e as peças jurídicas da base já etiquetada, com objetivo de replicar a etiqueta do processo mais parecido.\nExtratos de código relacionados a atividade de similaridade estão disponíveis no notebook Python. Maiores informações sobre similaridade podem ser encontradas neste link. A continuidade desta abordagem se tornou difícil pela heterogeneidade dos textos (petições em especial) e pela falta de textos etiquetados para a maioria dos ODS. Uma forma elegante de contornar este problema envolve quebrar o problema multilabel em problemas menores binários e será melhor explicada na seção seguinte."
  },
  {
    "objectID": "cap5.html#abordagem-binária-ods-16-8-10-e-3",
    "href": "cap5.html#abordagem-binária-ods-16-8-10-e-3",
    "title": "5  Abordagens e estratégias de classificação",
    "section": "5.3 Abordagem binária – ODS 16, 8, 10 e 3",
    "text": "5.3 Abordagem binária – ODS 16, 8, 10 e 3\nQuebrar o problema multilabel em problemas binários menores foi uma saída elegante que permitiu tratar cada ODS de forma individual, separando aqueles com número suficiente de processos daquelas com registros insuficientes para aprendizagem de máquina; Também permitiu ajustar um modelo para cada ODS, escolhendo os algoritmos candidatos em uma coleção maior de modelos. Como consequência de haver mais opções de modelos, surgiram também mais opções de ensemble, abordagem que reúne vários modelos e define o rótulo (etiqueta) por votação simples ou ponderada. Alguns Objetivos de Desenvolvimento Sustentável possuem poucos processos etiquetados, o que naturalmente impede que máquinas aprendam a classificá-los usando poucos exemplos. Se existem, por exemplo, 3 mil processos classificados e apenas 3 etiquetados em um determinado ODS, a máquina pode classificar nenhuma peça em tal ODS e ainda assim performar bem em termos de acurácia. Por este motivo, foram selecionados apenas os ODS 16, 8, 10 e 3, respectivamente os Objetivos de Desenvolvimento Sustentável com maiores números de entrada. Considerando cada um destes 4 ODS como um problema único de aprendizagem, a iniciativa RAFA alcançou resultados satisfatórios usando redes neurais. O fluxo de classificar processos usando a abordagem binária é o centro do projeto em R disponibilizado junto da documentação. Ao contrário da pasta zip que contém vários scripts para tarefas não conectadas e sem ordem definida, o projeto R é ponta a ponta, ou seja, começa com a leitura dos dados e termina com as classificações via redes neurais. A referência natural para usar redes neurais em linguagem R é Hvitfeldt e Silge (2021)1, livro base para o ajuste dos modelos utilizados no projeto R, cujos códigos dependem da conexão entre R e Python explicada na Seção 2.2. A iniciativa também usa redes neurais ajustadas através do pacote Pytorch, disponíveis no notebook Python.\nPodem ser utilizadas diversas arquiteturas de redes para NLP, em especial RNNs e LSTM2 e existem formas diferentes de processamento inicial do texto a depender da arquitetura escolhida. A principal etapa de pré-processamento é a de embedding, que se concentra na transformação de textos em vetores de números. Máquinas não processam textos nativamente, sendo necessário um passo de codificação, para transformar coleções de palavras em conjuntos de números. Existem diversas formas de embedding, sendo as mais simples baseadas em dicionário (conjunto de todas as palavras de um texto limpo) e as mais sofisticadas baseadas em contexto. Na construção dos modelos para predição de ODS foram utilizadas as seguintes tecnologias de embedding: one-hot encoding, bag of words (BoW), TF-IDF e BERT.3\nAs métricas tradicionais de aprendizagem de máquina são a acurácia, sensibilidade, especificidade, precisão e F-score. O quadro abaixo apresenta cada uma e suas respectivas fórmulas:\n\nEm que:  VP = Verdadeiros positivos; FN = Falsos negativos; FP = Falsos positivos; VN = Verdadeiros negativos; P = Precisão; S = Sensibilidade; N = Total de elementos.\nO uso de redes neurais se justifica pela boa performance nas métricas mais importantes de aprendizagem de máquina:\n\n\n\n\nTabela 2 – Métricas RAFA 2030\n \n  \n    ODS \n    Acurácia (%) \n    Sensibilidade (%) \n    F-Score (%) \n  \n \n\n  \n    ODS 16 \n    77,5 \n    85,3 \n    69,9 \n  \n  \n    ODS 8 \n    82,9 \n    84,1 \n    79,6 \n  \n  \n    ODS 10 \n    91,9 \n    100,0 \n    30,8 \n  \n  \n    ODS 3 \n    93,7 \n    83,3 \n    74,1 \n  \n\n\n\n\n\n\nCom a devida apresentação da etapa de classificação de processos em lotes, cumpre seguir com as explicações referentes ao aplicativo, desenvolvido para apoiar a atividade de classificação com poucas entradas, isto é, um processo por vez, tal qual os servidores do tribunal fazem de maneira manual. Este é o objetivo do capítulo a seguir."
  },
  {
    "objectID": "cap6.html",
    "href": "cap6.html",
    "title": "6  Aplicativo",
    "section": "",
    "text": "O aplicativo fica disponível em um link similar a um endereço web e pode ser utilizado em qualquer navegador (Google Chrome, Mozilla, etc), até mesmo em celulares. Nenhum pré-requisito técnico ou de hardware é necessário, pois o aplicativo foi criado para ser útil, funcional e intuitivo aos servidores da área jurídica. A primeira aba do aplicativo tem informações sobre o seu uso e a segunda contém informações sobre a Agenda 2030, com links oficiais para a ONU e para o hotsite do STF. O usuário encontra um campo para upload do processo na terceira aba (chamada RAFA 2030). Depois que o aplicativo lê a peça em PDF, aparecem sugestões de classificações obtidas via aprendizagem de máquina (ODS 16, 10, 8 e 3) e gráficos como a nuvem de palavras. Ainda nesta aba, o usuário pode baixar o texto limpo para utilização em algoritmos próprios. A figura a seguir mostra a aba RAFA 2030 do aplicativo:\n\n\n\nFigura 6.1: Aba RAFA 2030 do aplicativo\n\n\nAs demais abas mostram gráficos mais avançados, contagem e exibição das palavras-chave localizadas e um buscador de contexto, como mostram as figuras Figura 6.2 e Figura 6.3.\n\n\n\nFigura 6.2: Aba visualizações do aplicativo\n\n\n\n\n\nFigura 6.3: Aba contexto do aplicativo\n\n\nCom esses recursos é possível mapear palavras importantes do documento e analisar o contexto de cada uma delas. Um exemplo clássico é dado pela palavra “Salina”, que pode ser uma área para produção de sal marinho ou um sobrenome. Uma simples busca de contexto pode ajudar a não classificar um dado processo que contenha esta palavra em ODS relacionados à meio ambiente e economia sustentável, por exemplo. Vale o mesmo para a palavra “Rio”, que pode ser curso de água ou inicial de nomes de estados. Com o uso do aplicativo, analistas podem fazer um mapa geral do documento, de forma automatizada e em poucos segundos após o upload de uma peça jurídica. Espera-se que a ferramenta possa ajudar a diminuir o tempo para análise e classificação dos documentos, de forma a permitir que servidores sejam alocados em atividades menos manuais e repetitivas.\nAlguns módulos ainda podem ser incluídos e o aplicativo encontra-se em fase de testes por usuários do próprio tribunal. O aumento de processos para treinamento tende a trazer mudanças nas redes neurais, contudo não deve impactar no processo de classificação, já que em última análise, esta avaliação será feita por um servidor. Até a estabilização do algoritmo espera-se que o aplicativo seja de grande utilidade na padronização das classificações, o que gera um efeito cíclico de melhoria na performance dos algoritmos. Maiores informações sobre aplicativos em Shiny podem ser encontradas neste link."
  },
  {
    "objectID": "cap7.html",
    "href": "cap7.html",
    "title": "7  Códigos",
    "section": "",
    "text": "Notebook python\nProjeto R\nScripts doc"
  }
]